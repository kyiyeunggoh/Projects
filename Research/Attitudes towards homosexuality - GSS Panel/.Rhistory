}
}
View(distinct_con)
#Loop through rows of constituency and return a dataframe for all observations with names
output<-list()
foreach (i = 1:nrow(distinct_con)) %do% {
result<-get_latlong(distinct_con[i,3])
output[[i]]<-result
}
#Clean to ensure that calls ONLY UK data
distinct_con<-election_1920s%>%
distinct(Constituency)
View(distinct_con)
election_1920s<-rbind(election_1922,election_1923,election_1924,election_1929)
View(election_1920s)
View(election_1922)
View(election_1923)
election1920sdb <- readRDS("~/Downloads/Goh_KyiYeung/Final Project/election1920sdb.rds")
read_rds(file.choose(.))
rds<-read_rds(file.choose(.))
View(rds)
datalist=list()
for (i in 2:261) {
url<-"https://en.wikipedia.org/wiki/Constituency_election_results_in_the_1922_United_Kingdom_general_election"
wiki<- read_html(url)
wiki<-read_html(url)
table<-html_nodes(wiki,css=".wikitable")[i]
pagename_head<-'//*[@id="mw-content-text"]/div/table/caption'
pagename_year<-'//*[@id="firstHeading"]'
table_head<-html_nodes(wiki,xpath=pagename_head)[i]
table_year<-html_nodes(wiki,xpath=pagename_year)
results<-(html_table(table, fill=TRUE))[[1]]
constituency<-as.character(str_extract(html_text(table_head), "[[:alpha:]]+(?=\\d)"))
year<-as.numeric(str_extract(html_text(table_year), "(\\d)+"))
results['Constituency']=constituency
results['Year']=year
datalist[[i]]<-results
}
election_1922= do.call(rbind, datalist)
# Creating loop to extract 1923 data
datalist1=list()
for (i in 2:588) {
url<-"https://en.wikipedia.org/wiki/Constituency_election_results_in_the_1923_United_Kingdom_general_election"
wiki<-read_html(url)
table<-html_nodes(wiki,css=".wikitable")[i]
pagename_head<-'//*[@id="mw-content-text"]/div/table/caption'
pagename_year<-'//*[@id="firstHeading"]'
table_head<-html_nodes(wiki,xpath=pagename_head)[i]
table_year<-html_nodes(wiki,xpath=pagename_year)
results<-(html_table(table, fill=TRUE))[[1]]
constituency<-as.character(str_extract(html_text(table_head), "^[a-zA-Z]+"))
year<-as.numeric(str_extract(html_text(table_year), "(\\d)+"))
results['Constituency']=constituency
results['Year']=year
datalist1[[i]]<-results
}
election_1923= do.call(rbind, datalist1)
# Creating loop to extract 1929 data
datalist2=list()
for (i in 2:588) {
url<-"https://en.wikipedia.org/wiki/Constituency_election_results_in_the_1929_United_Kingdom_general_election"
wiki<-read_html(url)
table<-html_nodes(wiki,css=".wikitable")[i]
pagename_head<-'//*[@id="mw-content-text"]/div/table/caption'
pagename_year<-'//*[@id="firstHeading"]'
table_head<-html_nodes(wiki,xpath=pagename_head)[i]
table_year<-html_nodes(wiki,xpath=pagename_year)
results<-(html_table(table, fill=TRUE))[[1]]
constituency<-as.character(str_extract(html_text(table_head), "^[a-zA-Z]+"))
year<-as.numeric(str_extract(html_text(table_year), "(\\d)+"))
results['Constituency']=constituency
results['Year']=year
datalist2[[i]]<-results
}
election_1929= rbindlist(datalist2, fill=TRUE)
election_1929<-subset(election_1929, select=c(1:8))%>%
as.data.frame(.)
##NOTE: Function can be written but much faster to extract given that the structure of each page is different
##Hence, manual inspection of the xpaths are needed. I start at 2 because 1 is blank and end at 588 before
##data on university elections which are not relevant for our project. However, data from our second source,
##is far more amenable to extraction via a function and I have created one after testing it on a loop.
# Function to extract based on URL rather than manually keying 14 times of those pages for this website
polres_scrape<-function(url){
library(tidyverse)
library(stringr)
library(rvest)
listofdata<-list()
for (i in 1:length(read_html(url)%>%
html_nodes(.,css=".inner")%>%
as.list(.))) {
remwiki<-read_html(url)
tablerem<-html_nodes(remwiki, css=".inner")[i]
pagename_head<-html_nodes(remwiki,css=".constname")[i]
table_year<-as.numeric(str_extract(html_text(remwiki,'/html/body/h1'),"(\\d)+"))
results<-as.data.frame(html_table(tablerem, fill=TRUE))
constituency<-html_text(pagename_head)
year<-table_year
results["Constituency"]=as.character(constituency)
results['Year']=as.numeric(year)
results
listofdata[[i]]<-results
}
return(listofdata)
}
d1<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i01.html")
d2<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i02.html")
d3<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i03.html")
d4<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i04.html")
d5<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i05.html")
d6<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i06.html")
d7<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i07.html")
d8<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i08.html")
d9<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i09.html")
d10<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i10.html")
d11<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i11.html")
d12<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i12.html")
d13<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i13.html")
d14<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i14.html")
elections_1924<-bind_rows(d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11,d12,d13,d14)
## At this point in time, I will join the datasets together
election_1924<-elections_1924[c(2,1,3,4,5,6)]
colnames(election_1924)<-c("Party.1","Candidate","Votes","%","Constituency","Year")
election_1922<-election_1922[c(2,3,4,5,7,8)]
election_1923<-election_1923[c(2,3,4,5,7,8)]
election_1929<-election_1929[c(2,3,4,5,7,8)]
election_1920s<-rbind(election_1922,election_1923,election_1924,election_1929)
#Clean to ensure that calls ONLY UK data
distinct_con<-election_1920s%>%
distinct(Constituency)
distinct_con["Country"]=as.character(", United Kingdom")
distinct_con$Constituency<-str_replace(distinct_con$Constituency, "City", "City of London")
distinct_con$full <- paste(distinct_con$Constituency, distinct_con$Country)
## Extract constituency data and create dataframe for Google API calls using a function
get_latlong <- function(area, apiKey = Sys.getenv("GMAPS_APP_TOKEN")) {
library(tidyr)
parameters <- ""
if (!is.null(apiKey)) {
parameters <- str_c("&key=", apiKey)
}
apiRequests <- iconv(str_c("https://maps.googleapis.com/maps/api/geocode/json?address=", area , parameters), "", "UTF-8")
result <- c()
for(i in 1:length(area)) {
Sys.sleep(0.1)
conn <- httr::GET(URLencode(apiRequests[i]))
apiResponse <- jsonlite::fromJSON(httr::content(conn, "text"))
ac <- apiResponse$results$geometry$location
if (is.null(ac)) {
print("NULL: There appears to be problems in your entry.")
}
if (apiResponse$status != "OK") {
warning("status is not OK")
} else {
return(ac)
}
}
}
View(election_1922)
View(election_1923)
View(election_1922)
View(election_1923)
wiki<-read_html(url)
table<-html_nodes(wiki,css=".wikitable")[47]
table_head<-html_nodes(wiki,xpath=pagename_head)[i]
table_head<-html_nodes(wiki,xpath=pagename_head)[47]
constituency<-as.character(str_extract(html_text(table_head), "[[:alpha:]]+(?=\\d)"))
constituency
constituency<-as.character(str_extract(html_text(table_head), "^[a-zA-Z]+"))
constituency
table<-html_nodes(wiki,css=".wikitable")[48]
table_head<-html_nodes(wiki,xpath=pagename_head)[48]
constituency<-as.character(str_extract(html_text(table_head), "^[a-zA-Z]+"))
constituency
table<-html_nodes(wiki,css=".wikitable")[47]
table_head<-html_nodes(wiki,xpath=pagename_head)[47]
constituency<-as.character(str_extract(html_text(table_head), "^[a-zA-Z]+"))
constituency
table<-html_nodes(wiki,css=".wikitable")[49]
table_head<-html_nodes(wiki,xpath=pagename_head)[49]
constituency<-as.character(str_extract(html_text(table_head), "^[a-zA-Z]+"))
constituency
wiki<-read_html(url)
table<-html_nodes(wiki,css=".wikitable")
View(table)
wikis<-read_html(url)
table<-html_nodes(wiki,css=".wikitable")
url<-"https://en.wikipedia.org/wiki/Constituency_election_results_in_the_1922_United_Kingdom_general_election"
wikis<-read_html(url)
table<-html_nodes(wikis,css=".wikitable")
table_head<-html_nodes(wikis,xpath=pagename_head)[49]
constituency<-as.character(str_extract(html_text(table_head), "^[a-zA-Z]+"))
constituency
wikis<-read_html(url)
table<-html_nodes(wikis,css=".wikitable")
table_head<-html_nodes(wikis,xpath=pagename_head)[47]
constituency<-as.character(str_extract(html_text(table_head), "^[a-zA-Z]+"))
constituency
# Creating loop to extract 1922 data
datalist=list()
constituency<-as.character(str_extract(html_text(table_head), "^[a-zA-Z]"))
constituency
constituency<-as.character(str_extract(html_text(table_head), "[a-zA-Z]"))
constituency
table_head<-html_nodes(wikis,xpath=pagename_head)[47]
constituency<-as.character(str_extract(html_text(table_head), "[a-zA-Z]"))
constituency
constituency<-as.character(str_extract(html_text(table_head), "[:alpha:]"))
constituency
table_head<-html_nodes(wikis,xpath=pagename_head)[47]
constituency<-as.character(str_extract(html_text(table_head), "[:alpha:]"))
constituency
wikis<-read_html(url)
table<-html_nodes(wikis,css=".wikitable")
table_head<-html_nodes(wikis,xpath=pagename_head)[47]
constituency<-as.character(str_extract(html_text(table_head), "[:alpha:]"))
constituency
constituency<-as.character(str_extract(html_text(table_head), "[^a-zA-Z]*"))
constituency
constituency<-as.character(str_extract(html_text(table_head), "[^a-zA-Z]+"))
constituency
url<-"https://en.wikipedia.org/wiki/Constituency_election_results_in_the_1922_United_Kingdom_general_election"
wikis<-read_html(url)
table<-html_nodes(wikis,css=".wikitable")
table_head<-html_nodes(wikis,xpath=pagename_head)[47]
constituency<-as.character(str_extract(html_text(table_head), "[^a-zA-Z]+"))
constituency
url<-"https://en.wikipedia.org/wiki/Constituency_election_results_in_the_1922_United_Kingdom_general_election"
wikis<-read_html(url)
table<-html_nodes(wikis,css=".wikitable")
View(table)
View(table)
table_head<-html_nodes(wikis,xpath=pagename_head)[47]
table_head
constituency<-as.character(str_extract(html_text(table_head), "[^[:alpha:]]+"))
constituency
constituency<-as.character(str_extract(html_text(table_head), "[:alpha:]"))
constituency
constituency<-as.character(str_extract(html_text(table_head), "[:alpha:]+"))
constituency
constituency<-as.character(str_extract(html_text(table_head), "[:alpha:]+","\\1"))
constituency
constituency<-as.character(str_extract_all(html_text(table_head), "[:alpha:]+"))
constituency
constituency<-as.character(str_extract_all(html_text(table_head), "[A-Za-z]+"))
constituency
constituency<-as.character(str_extract_all(html_text(table_head), "^[A-Za-z]+"))
constituency
constituency<-as.character(str_extract_all(html_text(table_head), "^[A-Za-z]"))
constituency
constituency<-as.character(str_extract_all(html_text(table_head), "[[:alpha:]]+(?=\\d)"))
constituency
constituency<-as.character(str_extract_all(html_text(table_head), "[[:alpha:]]+(?=\\d)"))
constituency
table_head<-html_nodes(wikis,xpath=pagename_head)[47]
constituency<-as.character(str_extract_all(html_text(table_head), "[[:alpha:]]+(?=\\d)"))
constituency
table_head
constituency<-as.character(str_extract_all(html_text(table_head), "[[:alpha:]]+(?=\\d)"))
constituency
constituency<-as.character(str_extract_all(html_text(table_head), "[[:alpha:]]+"))
constituency
constituency<-str_extract_all(html_text(table_head), "[[:alpha:]]+"))
constituency
constituency<-as.character(str_extract(html_text(table_head), "[[:alpha:]]+"))
constituency
constituency<-as.character(str_extract(html_text(table_head), "[[:alpha:] ]+"))
constituency
library(rvest)
library(plyr)
library(dplyr)
library(tidyverse)
library(kableExtra)
library(stringr)
library(xml2)
library(httr)
library(data.table)
library(XML)
library(httr)
library(rebus)
library(RJSONIO)
library(googleway)
library(gtools)
library(ggmap)
library(foreach)
library(reshape)
# Creating loop to extract 1922 data
datalist=list()
for (i in 2:261) {
url<-"https://en.wikipedia.org/wiki/Constituency_election_results_in_the_1922_United_Kingdom_general_election"
wiki<- read_html(url)
wiki<-read_html(url)
table<-html_nodes(wiki,css=".wikitable")[i]
pagename_head<-'//*[@id="mw-content-text"]/div/table/caption'
pagename_year<-'//*[@id="firstHeading"]'
table_head<-html_nodes(wiki,xpath=pagename_head)[i]
table_year<-html_nodes(wiki,xpath=pagename_year)
results<-(html_table(table, fill=TRUE))[[1]]
constituency<-as.character(str_extract(html_text(table_head), "[[:alpha:] ]+"))
year<-as.numeric(str_extract(html_text(table_year), "(\\d)+"))
results['Constituency']=constituency
results['Year']=year
datalist[[i]]<-results
}
election_1922= do.call(rbind, datalist)
View(election_1922)
# Creating loop to extract 1923 data
datalist1=list()
for (i in 2:588) {
url<-"https://en.wikipedia.org/wiki/Constituency_election_results_in_the_1923_United_Kingdom_general_election"
wiki<-read_html(url)
table<-html_nodes(wiki,css=".wikitable")[i]
pagename_head<-'//*[@id="mw-content-text"]/div/table/caption'
pagename_year<-'//*[@id="firstHeading"]'
table_head<-html_nodes(wiki,xpath=pagename_head)[i]
table_year<-html_nodes(wiki,xpath=pagename_year)
results<-(html_table(table, fill=TRUE))[[1]]
constituency<-as.character(str_extract(html_text(table_head), "[[:alpha:] ]+"))
year<-as.numeric(str_extract(html_text(table_year), "(\\d)+"))
results['Constituency']=constituency
results['Year']=year
datalist1[[i]]<-results
}
election_1923= do.call(rbind, datalist1)
# Creating loop to extract 1929 data
datalist2=list()
for (i in 2:588) {
url<-"https://en.wikipedia.org/wiki/Constituency_election_results_in_the_1929_United_Kingdom_general_election"
wiki<-read_html(url)
table<-html_nodes(wiki,css=".wikitable")[i]
pagename_head<-'//*[@id="mw-content-text"]/div/table/caption'
pagename_year<-'//*[@id="firstHeading"]'
table_head<-html_nodes(wiki,xpath=pagename_head)[i]
table_year<-html_nodes(wiki,xpath=pagename_year)
results<-(html_table(table, fill=TRUE))[[1]]
constituency<-as.character(str_extract(html_text(table_head), "[[:alpha:] ]+"))
year<-as.numeric(str_extract(html_text(table_year), "(\\d)+"))
results['Constituency']=constituency
results['Year']=year
datalist2[[i]]<-results
}
election_1929= rbindlist(datalist2, fill=TRUE)
election_1929<-subset(election_1929, select=c(1:8))%>%
as.data.frame(.)
##NOTE: Function can be written but much faster to extract given that the structure of each page is different
##Hence, manual inspection of the xpaths are needed. I start at 2 because 1 is blank and end at 588 before
##data on university elections which are not relevant for our project. However, data from our second source,
##is far more amenable to extraction via a function and I have created one after testing it on a loop.
# Function to extract based on URL rather than manually keying 14 times of those pages for this website
polres_scrape<-function(url){
library(tidyverse)
library(stringr)
library(rvest)
listofdata<-list()
for (i in 1:length(read_html(url)%>%
html_nodes(.,css=".inner")%>%
as.list(.))) {
remwiki<-read_html(url)
tablerem<-html_nodes(remwiki, css=".inner")[i]
pagename_head<-html_nodes(remwiki,css=".constname")[i]
table_year<-as.numeric(str_extract(html_text(remwiki,'/html/body/h1'),"(\\d)+"))
results<-as.data.frame(html_table(tablerem, fill=TRUE))
constituency<-html_text(pagename_head)
year<-table_year
results["Constituency"]=as.character(constituency)
results['Year']=as.numeric(year)
results
listofdata[[i]]<-results
}
return(listofdata)
}
d1<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i01.html")
d2<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i02.html")
d3<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i03.html")
d4<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i04.html")
d5<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i05.html")
d6<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i06.html")
d7<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i07.html")
d8<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i08.html")
d9<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i09.html")
d10<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i10.html")
d11<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i11.html")
d12<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i12.html")
d13<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i13.html")
d14<-polres_scrape("http://www.politicsresources.net/area/uk/ge1924/i14.html")
elections_1924<-bind_rows(d1,d2,d3,d4,d5,d6,d7,d8,d9,d10,d11,d12,d13,d14)
## At this point in time, I will join the datasets together
election_1924<-elections_1924[c(2,1,3,4,5,6)]
colnames(election_1924)<-c("Party.1","Candidate","Votes","%","Constituency","Year")
election_1922<-election_1922[c(2,3,4,5,7,8)]
election_1923<-election_1923[c(2,3,4,5,7,8)]
election_1929<-election_1929[c(2,3,4,5,7,8)]
election_1920s<-rbind(election_1922,election_1923,election_1924,election_1929)
#Clean to ensure that calls ONLY UK data
distinct_con<-election_1920s%>%
distinct(Constituency)
distinct_con["Country"]=as.character(", United Kingdom")
distinct_con$Constituency<-str_replace(distinct_con$Constituency, "City", "City of London")
distinct_con$full <- paste(distinct_con$Constituency, distinct_con$Country)
## Extract constituency data and create dataframe for Google API calls using a function
get_latlong <- function(area, apiKey = Sys.getenv("GMAPS_APP_TOKEN")) {
library(tidyr)
parameters <- ""
if (!is.null(apiKey)) {
parameters <- str_c("&key=", apiKey)
}
apiRequests <- iconv(str_c("https://maps.googleapis.com/maps/api/geocode/json?address=", area , parameters), "", "UTF-8")
result <- c()
for(i in 1:length(area)) {
Sys.sleep(0.1)
conn <- httr::GET(URLencode(apiRequests[i]))
apiResponse <- jsonlite::fromJSON(httr::content(conn, "text"))
ac <- apiResponse$results$geometry$location
if (is.null(ac)) {
print("NULL: There appears to be problems in your entry.")
}
if (apiResponse$status != "OK") {
warning("status is not OK")
} else {
return(ac)
}
}
}
View(election_1920s)
View(distinct_con)
write_rds(election_1920s,"cleanelection1920s.rds")
#Loop through rows of constituency and return a dataframe for all observations with names
output<-list()
View(distinct_con)
foreach (i = 1:nrow(distinct_con)) %do% {
result<-get_latlong(distinct_con[i,3])
output[[i]]<-result
}
constituency_location<-bind_rows(output)
View(constituency_location)
write.csv(constituency_location,"constituency_location.rds")
View(output)
View(constituency_location)
View(distinct_con)
# Join Google API data with electoral data with new 'long/lat' descriptor
full_constituency<-cbind(distinct_con,constituency_location[1:697])
# Join Google API data with electoral data with new 'long/lat' descriptor
full_constituency<-cbind(distinct_con,constituency_location)
View(distinct_con)
# Join Google API data with electoral data with new 'long/lat' descriptor
full_constituency<-cbind(distinct_con,constituency_location[1:697])
constituency_location<-bind_rows(output)
# Join Google API data with electoral data with new 'long/lat' descriptor
full_constituency<-cbind(distinct_con,constituency_location[1:697])
View(constituency_location)
View(distinct_con)
# Join Google API data with electoral data with new 'long/lat' descriptor
full_constituency<-cbind(distinct_con,constituency_location[1:697,])
View(full_constituency)
write.rds(full_constituency,"full_constituency.rds")
write_rds(full_constituency,"full_constituency.rds")
clean_constituency_1920<-merge(election_1920s,full_constituency,by="Constituency")
View(clean_constituency_1920)
View(clean_constituency_1920)
election1920sdb<-clean_constituency_1920%>%
select(Constituency, Party.1, Candidate, Votes, "%", Year, lat, lng)
View(election1920sdb)
colnames(election1920sdb)[2]<-"Party"
colnames(election1920sdb)[5]<-"Percentages"
colnames(election1920sdb)[7]<-"Latitude"
colnames(election1920sdb)[8]<-"Longitude"
saveRDS(election1920sdb, file="election1920sdb.rds")
# Some descriptive statistics of the data
#Average vote swings for contested constituencies in 1920s
swingdata<-election1920sdb%>%
filter(str_detect(Votes,"Swing"))%>%
filter(str_detect(Percentages,"(?>-)*[[:digit:]]+\\.*[[:digit:]]*"))
swingdata$Percentages<-str_extract(swingdata$Percentages,"[0-9.]+")
mean(as.numeric(swingdata$Percentages))
#Most common candidate names
namesdata<-election1920sdb %>%
filter(!str_detect(election1920sdb$Candidate
,"Majority|Turnout|gain|hold|win|Electorate|Conservative|Labour|Liberal|
P|politician"))
bagofnames<-str_split(namesdata$Candidate," ")%>%
unlist(.)%>%
as.data.frame(.)
freq<-table(bagofnames)
maxFreq<-which.max(freq)
maxFreq
#Longest candidate names
which.max(nchar(namesdata$Candidate))
namesdata[2562,4]
#Longest candidate names
which.max(nchar(namesdata$Candidate))
namesdata[2622,3]
# Most votes in a constituency
rawvotes<-election1920sdb%>%
filter(!str_detect(Votes,"Swing"))%>%
filter(str_detect(Votes,"(?>-)*[[:digit:]]+\\.*[[:digit:]]*"))
rawvotes$Votes<-as.numeric(gsub("\\,", "", rawvotes$Votes))
max(as.numeric(rawvotes$Votes))
install.packages("electuk1920s")
install.packages("electuk1920s")
install.packages("electuk1920s")
library(electuk1920s)
get_latlong(electuk1920s)
get_latlong("columbia Uni")
election1920sdb <- readRDS("~/Downloads/Goh_KyiYeung/Final Project/electuk1920s/data/election1920sdb.rds")
View(election1920sdb)
election1920sdb%>%
View(election1920sdb)
# Some descriptive statistics of the data
#Average vote swings for contested constituencies in 1920s
swingdata<-election1920sdb%>%
filter(str_detect(Votes,"Swing"))%>%
filter(str_detect(Percentages,"(?>-)*[[:digit:]]+\\.*[[:digit:]]*"))
setwd("~/Desktop/Github Projects/Projects/Research/Attitudes towards homosexuality - GSS Panel")
setwd("~/Desktop/Github Projects/Projects/Research/Time Series Project")
